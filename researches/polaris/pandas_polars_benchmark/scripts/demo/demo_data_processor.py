#!/usr/bin/env python3

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json

# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –º–æ–¥—É–ª–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.reporting.data_processor import (
    DataProcessor, 
    MetricType, 
    AggregationLevel,
    ProcessedData
)
from utils.logging import get_logger


def create_sample_results():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞."""
    np.random.seed(42)
    
    libraries = ['pandas', 'polars']
    operations = ['read_csv', 'filter', 'groupby', 'sort', 'join']
    dataset_sizes = [1000, 10000, 100000, 1000000]
    
    results = []
    
    for lib in libraries:
        for op in operations:
            for size in dataset_sizes:
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                base_time = {
                    'pandas': {'read_csv': 0.1, 'filter': 0.05, 'groupby': 0.2, 'sort': 0.15, 'join': 0.3},
                    'polars': {'read_csv': 0.03, 'filter': 0.01, 'groupby': 0.05, 'sort': 0.04, 'join': 0.08}
                }
                
                # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞—Å—Ç–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö
                time_factor = np.log10(size) / 3
                base = base_time[lib][op]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
                for _ in range(5):
                    exec_time = base * time_factor * np.random.normal(1.0, 0.1)
                    memory_usage = size * 8 / 1024 / 1024 * np.random.normal(1.0, 0.15)  # MB
                    memory_peak = memory_usage * np.random.uniform(1.1, 1.5)
                    
                    results.append({
                        'library': lib,
                        'operation': op,
                        'dataset_size': size,
                        'execution_time': max(0.001, exec_time),
                        'memory_usage': max(1.0, memory_usage),
                        'memory_peak': max(1.0, memory_peak),
                        'timestamp': pd.Timestamp.now()
                    })
    
    return pd.DataFrame(results)


def demonstrate_comparison_preparation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    logger = get_logger(__name__)
    logger.info("="*60)
    logger.info("1. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–• –ì–†–ê–§–ò–ö–û–í")
    logger.info("="*60)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    df = create_sample_results()
    processor = DataProcessor()
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –æ–ø–µ—Ä–∞—Ü–∏—è–º
    comparison_data = processor.prepare_comparison_data(
        df=df,
        metric=MetricType.EXECUTION_TIME,
        groupby=['operation'],
        pivot_column='library'
    )
    
    print("\n–î–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫ –ø–æ –æ–ø–µ—Ä–∞—Ü–∏—è–º:")
    print(comparison_data.data)
    print(f"\n–ö–æ–ª–æ–Ω–∫–∏ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {comparison_data.data.columns.tolist()}")
    print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {comparison_data.metadata}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ä–∞—Å—á–µ—Ç–æ–º —É—Å–∫–æ—Ä–µ–Ω–∏—è
    speedup_data = processor.prepare_comparison_data(
        df=df,
        metric=MetricType.SPEEDUP,
        groupby=['operation', 'dataset_size'],
        pivot_column='library'
    )
    
    print("\n\n–î–∞–Ω–Ω—ã–µ –ø–æ —É—Å–∫–æ—Ä–µ–Ω–∏—é (Pandas –∫–∞–∫ baseline):")
    print(speedup_data.data.head(10))


def demonstrate_distribution_preparation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π."""
    logger = get_logger(__name__)
    logger.info("\n" + "="*60)
    logger.info("2. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ì–†–ê–§–ò–ö–û–í –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø")
    logger.info("="*60)
    
    df = create_sample_results()
    processor = DataProcessor()
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è Polars
    dist_data = processor.prepare_distribution_data(
        df=df,
        metric=MetricType.EXECUTION_TIME,
        library='polars',
        operation='groupby'
    )
    
    print("\n–î–∞–Ω–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è Polars (groupby):")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π: {len(dist_data.data)}")
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {dist_data.metadata['statistics']}")
    print("\n–ü–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π:")
    print(dist_data.data.head())


def demonstrate_timeline_preparation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤."""
    logger = get_logger(__name__)
    logger.info("\n" + "="*60)
    logger.info("3. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ì–†–ê–§–ò–ö–û–í –ó–ê–í–ò–°–ò–ú–û–°–¢–ò –û–¢ –†–ê–ó–ú–ï–†–ê")
    logger.info("="*60)
    
    df = create_sample_results()
    processor = DataProcessor()
    
    timeline_data = processor.prepare_timeline_data(
        df=df,
        metric=MetricType.EXECUTION_TIME,
        dataset_size_column='dataset_size'
    )
    
    print("\n–î–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞:")
    print(f"–†–∞–∑–º–µ—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {timeline_data.metadata['dataset_sizes']}")
    print(f"–û–ø–µ—Ä–∞—Ü–∏–∏: {timeline_data.metadata['operations']}")
    print("\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (filter –æ–ø–µ—Ä–∞—Ü–∏—è):")
    print(timeline_data.data[timeline_data.data['operation'] == 'filter'])


def demonstrate_heatmap_preparation():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ–ø–ª–æ–≤—ã—Ö –∫–∞—Ä—Ç."""
    logger = get_logger(__name__)
    logger.info("\n" + "="*60)
    logger.info("4. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –¢–ï–ü–õ–û–í–´–• –ö–ê–†–¢")
    logger.info("="*60)
    
    df = create_sample_results()
    processor = DataProcessor()
    
    heatmap_data = processor.prepare_heatmap_data(
        df=df,
        metric=MetricType.EXECUTION_TIME,
        row_column='operation',
        col_column='dataset_size'
    )
    
    print("\n–î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã:")
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π: {heatmap_data.metadata['value_range']}")
    print("\n–î–∞–Ω–Ω—ã–µ –¥–ª—è Pandas:")
    print(heatmap_data.data[heatmap_data.data['library'] == 'pandas'])


def demonstrate_summary_table():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã."""
    logger = get_logger(__name__)
    logger.info("\n" + "="*60)
    logger.info("5. –°–û–ó–î–ê–ù–ò–ï –°–í–û–î–ù–û–ô –¢–ê–ë–õ–ò–¶–´")
    logger.info("="*60)
    
    df = create_sample_results()
    processor = DataProcessor()
    
    summary = processor.create_summary_table(df)
    
    print("\n–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    print(summary.to_string(index=False, float_format='%.3f'))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞
    output_dir = Path("demo_output")
    output_dir.mkdir(exist_ok=True)
    
    summary.to_csv(output_dir / "summary_table.csv", index=False)
    print(f"\n–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir / 'summary_table.csv'}")


def demonstrate_export():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö."""
    logger = get_logger(__name__)
    logger.info("\n" + "="*60)
    logger.info("6. –≠–ö–°–ü–û–†–¢ –û–ë–†–ê–ë–û–¢–ê–ù–ù–´–• –î–ê–ù–ù–´–•")
    logger.info("="*60)
    
    df = create_sample_results()
    processor = DataProcessor()
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    processed = processor.prepare_comparison_data(
        df=df,
        metric=MetricType.EXECUTION_TIME,
        groupby=['operation'],
        pivot_column='library'
    )
    
    # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
    output_dir = Path("demo_output")
    output_dir.mkdir(exist_ok=True)
    
    json_path = output_dir / "comparison_data.json"
    processor.export_for_visualization(processed, json_path, format="json")
    
    # –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV
    csv_path = output_dir / "comparison_data.csv"
    processor.export_for_visualization(processed, csv_path, format="csv")
    
    print(f"–î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤:")
    print(f"  - JSON: {json_path}")
    print(f"  - CSV: {csv_path}")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ JSON
    with open(json_path, 'r') as f:
        json_content = json.load(f)
    print(f"\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON:")
    print(f"  - –ö–ª—é—á–∏: {list(json_content.keys())}")
    print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(json_content['data'])}")
    print(f"  - –£—Ä–æ–≤–µ–Ω—å –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {json_content['aggregation_level']}")
    print(f"  - –¢–∏–ø –º–µ—Ç—Ä–∏–∫–∏: {json_content['metric_type']}")


if __name__ == "__main__":
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö\n")
    
    # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π
    demonstrate_comparison_preparation()
    demonstrate_distribution_preparation()
    demonstrate_timeline_preparation()
    demonstrate_heatmap_preparation()
    demonstrate_summary_table()
    demonstrate_export()
    
    print("\n‚ú® –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
